{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### THIS FILE CONTAINS CONFIDENTIAL DATA, ONLY RAW CODE HAS BEEN UPLOADED. INDIVIDUAL CELL OUTPUTS HAVE BEEN OMITTED. ANONYMIZATION KEY AND RAW VIEWERSHIP DATA WILL NOT BE LOADED TO DATABASE AND UNAVAILABLE. THIS CODE WILL DOCUMENT THE CLEANING FOR THE VIEWERSHIP DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging multiple CSVs to one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data Processing Into DataFrames\n",
    "# Import dependancies\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta, date\n",
    "from pathlib import Path\n",
    "import pycountry_convert as pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging Minute level aggregation into one table\n",
    "\n",
    "#Determine input date range\n",
    "start_date = \"2021-02-18\"\n",
    "end_date = \"2022-11-06\"\n",
    "\n",
    "#string to datetime\n",
    "start_date_dt = datetime.strptime(start_date, '%Y-%m-%d').date()\n",
    "end_date_dt = datetime.strptime(end_date, '%Y-%m-%d').date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that will read all the csv files and combine them into one dataframe.\n",
    "\n",
    "def minute_data_aggregation_condenser(start, end):\n",
    "    current_date = start\n",
    "    combined_df = pd.DataFrame(pd.read_csv(f\"Resources/minutelevelsessionaggregations-qwest-{start_date_dt}.csv\"))\n",
    "    counter = 1\n",
    "\n",
    "    while current_date <= end:\n",
    "        try:\n",
    "            current_date = current_date + timedelta(days=1)\n",
    "            current_data = pd.read_csv(f\"Resources/minutelevelsessionaggregations-qwest-{current_date}.csv\")\n",
    "            current_df = pd.DataFrame(current_data)\n",
    "            combined_df = pd.concat([combined_df, current_df])\n",
    "            counter += 1\n",
    "        except:\n",
    "            current_date = current_date + timedelta(days=1)\n",
    "    else:\n",
    "        print(f\"Data Merge Complete, {counter} files have been merged into a dataframe and exported as merged_aggregate_data{date.today()}.csv\")\n",
    "        return combined_df\n",
    "\n",
    "combined_df = minute_data_aggregation_condenser(start_date_dt, end_date_dt)\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check df length\n",
    "print(len(combined_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check columns in df\n",
    "combined_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extra column named channel;time;content_id;country;total_sessions;total_session_duration_seconds indicates error\n",
    "# Check that column\n",
    "len(combined_df.columns)\n",
    "combined_df['channel;time;content_id;country;total_sessions;total_session_duration_seconds'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all rows containing bad data\n",
    "combined_df = combined_df[pd.isnull(combined_df['channel;time;content_id;country;total_sessions;total_session_duration_seconds'])]\n",
    "\n",
    "# Column was semicolon separated rather than comma separated on 2022-06-26\n",
    "semicolon_data = pd.read_csv(\"Resources/minutelevelsessionaggregations-qwest-2022-06-26.csv\", sep=';')\n",
    "semicolon_df = pd.DataFrame(semicolon_data)\n",
    "semicolon_df.head()\n",
    "\n",
    "#combine with complete dataframe\n",
    "combined_df=pd.concat([combined_df, semicolon_df])\n",
    "\n",
    "#remove 'channel;time;content_id;country;total_sessions;total_session_duration_seconds' column\n",
    "combined_df = combined_df.drop(columns=['channel;time;content_id;country;total_sessions;total_session_duration_seconds'])\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check length of df to make sure it matches with previous \n",
    "print(len(combined_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anonymizing Key and Channel Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import anonymization key\n",
    "anon_key = pd.read_csv(\"Resources/Anonymization Keys.csv\")\n",
    "anon_key_df = pd.DataFrame(anon_key)\n",
    "anon_key_df\n",
    "\n",
    "anon_key_op_df = anon_key_df[['Operator', 'anonymization key']]\n",
    "anon_key_chan_df = anon_key_df[['Channel', 'Anonymization key (Genre)']]\n",
    "anon_key_chan_df = anon_key_chan_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#match channel with operator ### EXPECTED TIME 349minutes REFACTOR THIS CODE IN THE FUTURE TO RUN LIKE THE CONTENT_ID\n",
    "def string_parser_OPS (string):\n",
    "    for ops in anon_key_op_df['Operator']:\n",
    "        if string.str.contains(ops.lower()).any():\n",
    "            return anon_key_op_df.loc[anon_key_op_df['Operator']== ops, 'anonymization key'].item()\n",
    "\n",
    "combined_df[\"Operator\"] = combined_df[['channel']].apply(string_parser_OPS, axis =1, result_type='expand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#match channel with channel ### EXPECTED TIME 244minutes REFACTOR THIS CODE IN THE FUTURE TO RUN LIKE THE CONTENT_ID\n",
    "def string_parser_CHAN (string):\n",
    "    for ops in anon_key_chan_df['Channel']:\n",
    "        if string.str.contains(ops.lower()).any():\n",
    "            return anon_key_chan_df.loc[anon_key_chan_df['Channel']== ops, 'Anonymization key (Genre)'].item()\n",
    "\n",
    "combined_df[\"Channel\"] = combined_df[['channel']].apply(string_parser_CHAN, axis =1, result_type='expand')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anonymizing Program Code and obtaining Genre "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_data = pd.read_csv('Resources/Media Library.csv')\n",
    "content_df = pd.DataFrame(content_data)\n",
    "\n",
    "# Add PRO_ prefix to ID to get Program ID\n",
    "content_df['PRO_CONTENT_ID'] = 'PRO_' + content_df['ðŸŽ¦  ID']\n",
    "#invert rows sort by largets to smallest so that PRO_3000 is found as PRO_3000 instead of PRO_3\n",
    "content_df = content_df.sort_index(ascending=False)\n",
    "content_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "playlist_data = pd.read_csv('Resources/Playlist With Music Tags.csv')\n",
    "playlist_df = pd.DataFrame(playlist_data)\n",
    "# drop to just name and genre\n",
    "playlist_df = playlist_df[['Name','ðŸŽ¯  TAG Music Styles']]\n",
    "playlist_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate unique playlist IDs and Anonymize\n",
    "\n",
    "#separate string and number from title\n",
    "unique_playlists = playlist_df['Name'].str.extract(r'([a-z]*)([0-9]*)')\n",
    "\n",
    "# find unique values in column containing text and make an index number to make a key\n",
    "unique_playlist_letters = pd.DataFrame(unique_playlists[0].unique())\n",
    "unique_playlist_letters['index_key'] = unique_playlist_letters.index\n",
    "\n",
    "# merge on letters\n",
    "keyed_text_playlist = (unique_playlists.merge(unique_playlist_letters, left_on=0, right_on=0))\n",
    "\n",
    "# Create anonymization key\n",
    "playlist_df['anonymized_key'] = ('PLY_') + (keyed_text_playlist['index_key'].astype(str)) + (keyed_text_playlist[1].astype(str))\n",
    "playlist_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out content_id with regex to get program number\n",
    "regex_list = [r'(PRO_*\\d*)_[A-Z]',r'(PRO_*\\d*\\w*)', r'pro(\\d{1,4})', r'pro_(\\d*\\w*)',r'(^\\d{1,4})[a-z]',r'pr\\d*[a-z]*(\\d*)']\n",
    "regex_filtered_content_id = combined_df.content_id.str.extract('|'.join(regex_list))\n",
    "# add PRO_ prefix to extracted numbers\n",
    "for i in range(len(regex_list)-1, 1, -1):\n",
    "        regex_filtered_content_id[i]='PRO_' + regex_filtered_content_id[i]\n",
    "#Merge all columns\n",
    "for i in range(len(regex_list), 0, -1):\n",
    "    if i-2 >= 0:\n",
    "        regex_filtered_content_id[i-2] = regex_filtered_content_id[i-2].fillna(regex_filtered_content_id[i-1])\n",
    "regex_filtered_content_id = regex_filtered_content_id[[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add column to the dataframe\n",
    "combined_df['filtered_content_id'] = regex_filtered_content_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge (VLOOKUP) playlists on exact title but dont drop rows\n",
    "combined_df = combined_df.merge(playlist_df, how='left', left_on='content_id', right_on='Name')\n",
    "combined_df = combined_df.drop(['Name'], axis=1)\n",
    "\n",
    "#merge the filtered id for programs with anonymized id for playlists then drop anonymized key column\n",
    "combined_df['filtered_content_id'] = combined_df['filtered_content_id'].fillna(combined_df['anonymized_key'])\n",
    "combined_df=combined_df.drop(['anonymized_key'], axis=1)\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the content_id of rows that have nan for filtered_content_id\n",
    "missed_content_ids = combined_df[combined_df['filtered_content_id'].isnull()]\n",
    "missed_content_ids = missed_content_ids[['content_id']]\n",
    "missing_content_id_df = pd.DataFrame(missed_content_ids['content_id'].unique())\n",
    "missing_content_id_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge (Vlookup) programs with viewership data\n",
    "combined_genre_df = (combined_df.merge(content_df, left_on='filtered_content_id', right_on='PRO_CONTENT_ID'))\n",
    "\n",
    "# merge tag columns containing genre\n",
    "combined_genre_df['ðŸŽ¯  TAG Music Styles'] = combined_genre_df['ðŸŽ¯  TAG Music Styles'].fillna(combined_genre_df['ðŸŽ¯  TAG Music Styles (from ðŸŽ¥ Films)'])\n",
    "\n",
    "# drop rows where genre is nan\n",
    "combined_genre_df = combined_genre_df[combined_genre_df['ðŸŽ¯  TAG Music Styles'].notna()]\n",
    "\n",
    "combined_genre_df.head()\n",
    "# 80.38% Match Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trim useless columns out, and fix column names\n",
    "trimmed_clean_merged_minute_aggregation = combined_genre_df[['time', 'country', 'total_sessions', 'total_session_duration_seconds', 'Operator', 'Channel', 'filtered_content_id', 'ðŸŽ¯  TAG Music Styles (from ðŸŽ¥ Films)']]\n",
    "trimmed_clean_merged_minute_aggregation = trimmed_clean_merged_minute_aggregation.rename({'time':'time', 'country':'country', 'total_sessions':'total_sessions', 'total_session_duration_seconds':'total_session_duration_seconds', 'Operator':'operator', 'Channel':'channel','filtered_content_id':'cleaned_content_id', 'ðŸŽ¯  TAG Music Styles (from ðŸŽ¥ Films)':'genre'}, axis='columns')\n",
    "trimmed_clean_merged_minute_aggregation.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert time to datetime\n",
    "trimmed_clean_merged_minute_aggregation['time']=pd.to_datetime(trimmed_clean_merged_minute_aggregation['time'])\n",
    "trimmed_clean_merged_minute_aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check data types\n",
    "trimmed_clean_merged_minute_aggregation.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding Region Code By Bucketing Countries Into Regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add region code by bucketting into regions\n",
    "\n",
    "# Bucketing countries into regions\n",
    "# Conversion Function\n",
    "def convert(row):\n",
    "    try:\n",
    "        country_code = pc.country_name_to_country_alpha2(row.country, cn_name_format = \"default\")\n",
    "        continent_code = pc.country_alpha2_to_continent_code(country_code)\n",
    "        return continent_code\n",
    "    except:\n",
    "        print(f'{row.country} not found')\n",
    "\n",
    "# Changing the Country Name so it doesnt interfere with the function\n",
    "trimmed_clean_merged_minute_aggregation['country'] = trimmed_clean_merged_minute_aggregation['country'].replace(\"Kosovo\", \"Albania\")\n",
    "trimmed_clean_merged_minute_aggregation['country'] = trimmed_clean_merged_minute_aggregation['country'].replace(\"Runion\", \"RÃ©union\")\n",
    "trimmed_clean_merged_minute_aggregation['country'] = trimmed_clean_merged_minute_aggregation['country'].replace(\"U.S. Virgin Islands\", \"Others\")\n",
    "trimmed_clean_merged_minute_aggregation['country'] = trimmed_clean_merged_minute_aggregation['country'].replace(\"Western Sahara\", \"Morocco\")\n",
    "trimmed_clean_merged_minute_aggregation['country'] = trimmed_clean_merged_minute_aggregation['country'].replace(\"Vatican City\", \"Italy\")\n",
    "trimmed_clean_merged_minute_aggregation['country'] = trimmed_clean_merged_minute_aggregation['country'].replace(\"St Vincent and Grenadines\", \"Saint Vincent and the Grenadines\")\n",
    "trimmed_clean_merged_minute_aggregation['country'] = trimmed_clean_merged_minute_aggregation['country'].replace(\"Timor-Leste\", \"Others\")\n",
    "trimmed_clean_merged_minute_aggregation['country'] = trimmed_clean_merged_minute_aggregation['country'].replace(\"St Kitts and Nevis\", \"the Federation of Saint Christopher and Nevis\")\n",
    "trimmed_clean_merged_minute_aggregation['country'] = trimmed_clean_merged_minute_aggregation['country'].replace(\"So Tom and Prncipe\", \"Others\")\n",
    "trimmed_clean_merged_minute_aggregation['country'] = trimmed_clean_merged_minute_aggregation['country'].replace(\"Sint Maarten\", \"Saint Martin\")\n",
    "trimmed_clean_merged_minute_aggregation['country'] = trimmed_clean_merged_minute_aggregation['country'].replace(\"Saint Barthlemy\", \"Others\")\n",
    "trimmed_clean_merged_minute_aggregation['country'] = trimmed_clean_merged_minute_aggregation['country'].replace(\"Bonaire Sint Eustatius and Saba\", \"Others\")\n",
    "trimmed_clean_merged_minute_aggregation['country'] = trimmed_clean_merged_minute_aggregation['country'].replace(\"DR Congo\", \"Democratic Republic of the Congo\")\n",
    "trimmed_clean_merged_minute_aggregation['country'] = trimmed_clean_merged_minute_aggregation['country'].replace(\"Curaao\", \"CuraÃ§ao\")\n",
    "trimmed_clean_merged_minute_aggregation['country'] = trimmed_clean_merged_minute_aggregation['country'].replace(\"Ã…land\", \"Finland\")\n",
    "\n",
    "# Create a [region] column based on country names with pycountry\n",
    "trimmed_clean_merged_minute_aggregation['region'] = trimmed_clean_merged_minute_aggregation.apply(convert, axis=1)\n",
    "\n",
    "#getting Unique Regions\n",
    "trimmed_clean_merged_minute_aggregation['region'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping continent codes to continent names \n",
    "continent_names = { 'NA' : 'North America',\n",
    "                    'AS' : 'Asia',\n",
    "                    'EU' : 'Europe',\n",
    "                    'SA' : 'South America',\n",
    "                    'AF' : 'Africa',\n",
    "                    'OC' : 'Oceania'}\n",
    "\n",
    "trimmed_clean_merged_minute_aggregation['region'] = trimmed_clean_merged_minute_aggregation['region'].map(continent_names)\n",
    "trimmed_clean_merged_minute_aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exporting final cleaned CSV and missing content_ids for further data cleaning if necesary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output new dataframe to CSV\n",
    "filepath = Path(f'Resources/cleaned_merged_minute_aggregate_data-{date.today()}.csv')\n",
    "trimmed_clean_merged_minute_aggregation.to_csv(filepath)\n",
    "\n",
    "# Output problem content_id dataframe to CSV\n",
    "filepath = Path(f'Resources/problematic_content_id.csv')\n",
    "missing_content_id_df.to_csv(filepath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('ML-GPU')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "448edaee4350f241a20dd5523d676348370318c49887029613f9e42feaa6bcd3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
