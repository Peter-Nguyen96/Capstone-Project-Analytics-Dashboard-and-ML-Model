{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### THIS FILE CONTAINS CONFIDENTIAL DATA, ONLY RAW CODE HAS BEEN UPLOADED. INDIVIDUAL CELL OUTPUTS HAVE BEEN OMITTED. ANONYMIZATION KEY AND RAW VIEWERSHIP DATA WILL NOT BE LOADED TO DATABASE AND UNAVAILABLE. THIS CODE WILL DOCUMENT THE CLEANING FOR THE VIEWERSHIP DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging multiple CSVs to one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data Processing Into DataFrames\n",
    "# Import dependancies\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta, date\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging Minute level aggregation into one table\n",
    "\n",
    "#Determine input date range\n",
    "start_date = \"2021-02-18\"\n",
    "end_date = \"2022-11-06\"\n",
    "\n",
    "#string to datetime\n",
    "start_date_dt = datetime.strptime(start_date, '%Y-%m-%d').date()\n",
    "end_date_dt = datetime.strptime(end_date, '%Y-%m-%d').date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that will read all the csv files and combine them into one dataframe.\n",
    "\n",
    "def minute_data_aggregation_condenser(start, end):\n",
    "    current_date = start\n",
    "    combined_df = pd.DataFrame(pd.read_csv(f\"Resources/minutelevelsessionaggregations-qwest-{start_date_dt}.csv\"))\n",
    "    counter = 1\n",
    "\n",
    "    while current_date <= end:\n",
    "        try:\n",
    "            current_date = current_date + timedelta(days=1)\n",
    "            current_data = pd.read_csv(f\"Resources/minutelevelsessionaggregations-qwest-{current_date}.csv\")\n",
    "            current_df = pd.DataFrame(current_data)\n",
    "            combined_df = pd.concat([combined_df, current_df])\n",
    "            counter += 1\n",
    "        except:\n",
    "            current_date = current_date + timedelta(days=1)\n",
    "    else:\n",
    "        print(f\"Data Merge Complete, {counter} files have been merged into a dataframe and exported as merged_aggregate_data{date.today()}.csv\")\n",
    "        return combined_df\n",
    "\n",
    "combined_df = minute_data_aggregation_condenser(start_date_dt, end_date_dt)\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check df length\n",
    "print(len(combined_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check columns in df\n",
    "combined_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extra column named channel;time;content_id;country;total_sessions;total_session_duration_seconds indicates error\n",
    "# Check that column\n",
    "len(combined_df.columns)\n",
    "combined_df['channel;time;content_id;country;total_sessions;total_session_duration_seconds'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all rows containing bad data\n",
    "combined_df = combined_df[pd.isnull(combined_df['channel;time;content_id;country;total_sessions;total_session_duration_seconds'])]\n",
    "\n",
    "# Column was semicolon separated rather than comma separated on 2022-06-26\n",
    "semicolon_data = pd.read_csv(\"Resources/minutelevelsessionaggregations-qwest-2022-06-26.csv\", sep=';')\n",
    "semicolon_df = pd.DataFrame(semicolon_data)\n",
    "semicolon_df.head()\n",
    "\n",
    "#combine with complete dataframe\n",
    "combined_df=pd.concat([combined_df, semicolon_df])\n",
    "\n",
    "#remove 'channel;time;content_id;country;total_sessions;total_session_duration_seconds' column\n",
    "combined_df = combined_df.drop(columns=['channel;time;content_id;country;total_sessions;total_session_duration_seconds'])\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check length of df to make sure it matches with previous \n",
    "print(len(combined_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anonymizing Key and Channel Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import anonymization key\n",
    "anon_key = pd.read_csv(\"Resources/Anonymization Keys.csv\")\n",
    "anon_key_df = pd.DataFrame(anon_key)\n",
    "anon_key_df\n",
    "\n",
    "anon_key_op_df = anon_key_df[['Operator', 'anonymization key']]\n",
    "anon_key_chan_df = anon_key_df[['Channel', 'Anonymization key (Genre)']]\n",
    "anon_key_chan_df = anon_key_chan_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#match channel with operator ### EXPECTED TIME 349minutes REFACTOR THIS CODE IN THE FUTURE TO RUN LIKE THE CONTENT_ID\n",
    "def string_parser_OPS (string):\n",
    "    for ops in anon_key_op_df['Operator']:\n",
    "        if string.str.contains(ops.lower()).any():\n",
    "            return anon_key_op_df.loc[anon_key_op_df['Operator']== ops, 'anonymization key'].item()\n",
    "\n",
    "combined_df[\"Operator\"] = combined_df[['channel']].apply(string_parser_OPS, axis =1, result_type='expand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#match channel with channel ### EXPECTED TIME 244minutes REFACTOR THIS CODE IN THE FUTURE TO RUN LIKE THE CONTENT_ID\n",
    "def string_parser_CHAN (string):\n",
    "    for ops in anon_key_chan_df['Channel']:\n",
    "        if string.str.contains(ops.lower()).any():\n",
    "            return anon_key_chan_df.loc[anon_key_chan_df['Channel']== ops, 'Anonymization key (Genre)'].item()\n",
    "\n",
    "combined_df[\"Channel\"] = combined_df[['channel']].apply(string_parser_CHAN, axis =1, result_type='expand')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anonymizing Program Code and obtaining Genre "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_data = pd.read_csv('Resources/Media Library.csv')\n",
    "content_df = pd.DataFrame(content_data)\n",
    "\n",
    "# Add PRO_ prefix to ID to get Program ID\n",
    "content_df['PRO_CONTENT_ID'] = 'PRO_' + content_df['ðŸŽ¦  ID']\n",
    "#invert rows sort by largets to smallest so that PRO_3000 is found as PRO_3000 instead of PRO_3\n",
    "content_df = content_df.sort_index(ascending=False)\n",
    "content_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out content_id with regex to get program number\n",
    "regex_list = [r'(PRO_*\\d*)_[A-Z]',r'(PRO_*\\d*\\w*)', r'pro(\\d{1,4})', r'pro_(\\d*\\w*)',r'(^\\d{1,4})[a-z]',r'pr\\d*[a-z]*(\\d*)']\n",
    "regex_filtered_content_id = combined_df.content_id.str.extract('|'.join(regex_list))\n",
    "# add PRO_ prefix to extracted numbers\n",
    "for i in range(len(regex_list)-1, 1, -1):\n",
    "        regex_filtered_content_id[i]='PRO_' + regex_filtered_content_id[i]\n",
    "#Merge all columns\n",
    "for i in range(len(regex_list), 0, -1):\n",
    "    if i-2 >= 0:\n",
    "        regex_filtered_content_id[i-2] = regex_filtered_content_id[i-2].fillna(regex_filtered_content_id[i-1])\n",
    "regex_filtered_content_id = regex_filtered_content_id[[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add column to the dataframe\n",
    "combined_df['filtered_content_id'] = regex_filtered_content_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the content_id of rows that have nan for filtered_content_id\n",
    "missed_content_ids = combined_df[combined_df['filtered_content_id'].isnull()]\n",
    "missed_content_ids = missed_content_ids[['content_id']]\n",
    "missing_content_id_df = pd.DataFrame(missed_content_ids['content_id'].unique())\n",
    "missing_content_id_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge and combined (VLOOKUP)\n",
    "combined_genre_df = (combined_df.merge(content_df, left_on='filtered_content_id', right_on='PRO_CONTENT_ID'))\n",
    "combined_genre_df.head()\n",
    "# 69.65% Match Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trim useless columns out, and fix column names\n",
    "trimmed_clean_merged_minute_aggregation = combined_genre_df[['time', 'country', 'total_sessions', 'total_session_duration_seconds', 'Operator', 'Channel', 'filtered_content_id', 'ðŸŽ¯  TAG Music Styles (from ðŸŽ¥ Films)']]\n",
    "trimmed_clean_merged_minute_aggregation = trimmed_clean_merged_minute_aggregation.rename({'time':'Time', 'country':'Country', 'total_sessions':'Total_Sessions', 'total_session_duration_seconds':'Total_Session_Duration_Seconds', 'Operator':'Operator', 'Channel':'Channel','filtered_content_id':'Cleaned_Content_ID', 'ðŸŽ¯  TAG Music Styles (from ðŸŽ¥ Films)':'Genre'}, axis='columns')\n",
    "trimmed_clean_merged_minute_aggregation.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exporting final cleaned CSV and missing content_ids for further data cleaning if necesary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output new dataframe to CSV\n",
    "filepath = Path(f'Resources/cleaned_merged_minute_aggregate_data-{date.today()}.csv')\n",
    "trimmed_clean_merged_minute_aggregation.to_csv(filepath)\n",
    "\n",
    "# Output problem content_id dataframe to CSV\n",
    "filepath = Path(f'Resources/problematic_content_id.csv')\n",
    "missing_content_id_df.to_csv(filepath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('ML-GPU')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "448edaee4350f241a20dd5523d676348370318c49887029613f9e42feaa6bcd3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
