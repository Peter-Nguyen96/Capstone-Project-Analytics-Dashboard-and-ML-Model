{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, OneHotEncoder\n",
    "from tensorflow_addons.metrics import RSquare\n",
    "\n",
    "import findspark\n",
    "# Start Spark Session\n",
    "findspark.init('C:\\Spark\\spark-3.2.2-bin-hadoop2.7')\n",
    "import pyspark\n",
    "from pyspark import SparkFiles\n",
    "from pyspark.sql.functions import to_date, to_timestamp\n",
    "from getpass import getpass\n",
    "\n",
    "\n",
    "# Build Spark Session\n",
    "### IMPORTANT: MAKE SURE THAT spark.config has memory set to 32GB or larger!\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Qwest-Analytics-Dashboard-and-ML-Model\").config(\"spark.jars\", \"C:\\Spark\\spark-3.2.2-bin-hadoop2.7\\jars\\postgresql-42.5.0.jar\").getOrCreate()\n",
    "\n",
    "#Store environmental variables\n",
    "password = getpass('Enter DataBase Password: ')\n",
    "\n",
    "#Configure for RDS\n",
    "jdbc_url=\"jdbc:postgresql://qwest-final-project.ccngkdwtiuvz.us-east-2.rds.amazonaws.com:5432/Qwest-Database\"\n",
    "config = {\"user\":\"postgres\", \n",
    "          \"password\": password, \n",
    "          \"driver\":\"org.postgresql.Driver\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------+--------------+------------------------------+--------+-----------+------------------+--------------------+-------------+\n",
      "|               time|       country|total_sessions|total_session_duration_seconds|operator|    channel|cleaned_content_id|               genre|       region|\n",
      "+-------------------+--------------+--------------+------------------------------+--------+-----------+------------------+--------------------+-------------+\n",
      "|2022-02-19 08:07:00| United States|           2.0|                         120.0|  Op_006|Channel_002|          PRO_1334|ORCHESTRA, CLASSICAL|North America|\n",
      "|2022-02-19 08:07:00|       Finland|           1.0|                          60.0|  Op_006|Channel_002|          PRO_1334|ORCHESTRA, CLASSICAL|       Europe|\n",
      "|2022-02-19 08:07:00|United Kingdom|           1.0|                          60.0|  Op_006|Channel_002|          PRO_1334|ORCHESTRA, CLASSICAL|       Europe|\n",
      "|2022-02-19 08:08:00|       Finland|           1.0|                          60.0|  Op_006|Channel_002|          PRO_1334|ORCHESTRA, CLASSICAL|       Europe|\n",
      "|2022-02-19 08:08:00| United States|           2.0|                         120.0|  Op_006|Channel_002|          PRO_1334|ORCHESTRA, CLASSICAL|North America|\n",
      "|2022-02-19 08:08:00|United Kingdom|           1.0|                          60.0|  Op_006|Channel_002|          PRO_1334|ORCHESTRA, CLASSICAL|       Europe|\n",
      "|2022-02-19 08:09:00|United Kingdom|           1.0|                          60.0|  Op_006|Channel_002|          PRO_1334|ORCHESTRA, CLASSICAL|       Europe|\n",
      "|2022-02-19 08:09:00|       Finland|           1.0|                          60.0|  Op_006|Channel_002|          PRO_1334|ORCHESTRA, CLASSICAL|       Europe|\n",
      "|2022-02-19 08:09:00| United States|           2.0|                         120.0|  Op_006|Channel_002|          PRO_1334|ORCHESTRA, CLASSICAL|North America|\n",
      "|2022-02-19 08:10:00|       Finland|           1.0|                          60.0|  Op_006|Channel_002|          PRO_1334|ORCHESTRA, CLASSICAL|       Europe|\n",
      "|2022-02-19 08:10:00| United States|           2.0|                         120.0|  Op_006|Channel_002|          PRO_1334|ORCHESTRA, CLASSICAL|North America|\n",
      "|2022-02-19 08:10:00|United Kingdom|           1.0|                          60.0|  Op_006|Channel_002|          PRO_1334|ORCHESTRA, CLASSICAL|       Europe|\n",
      "|2022-02-19 08:11:00|       Finland|           1.0|                          60.0|  Op_006|Channel_002|          PRO_1334|ORCHESTRA, CLASSICAL|       Europe|\n",
      "|2022-02-19 08:11:00| United States|           2.0|                         120.0|  Op_006|Channel_002|          PRO_1334|ORCHESTRA, CLASSICAL|North America|\n",
      "|2022-02-19 08:11:00|United Kingdom|           1.0|                          60.0|  Op_006|Channel_002|          PRO_1334|ORCHESTRA, CLASSICAL|       Europe|\n",
      "|2022-02-19 08:12:00|United Kingdom|           1.0|                          60.0|  Op_006|Channel_002|          PRO_1334|ORCHESTRA, CLASSICAL|       Europe|\n",
      "|2022-02-19 08:12:00|       Finland|           1.0|                          60.0|  Op_006|Channel_002|          PRO_1334|ORCHESTRA, CLASSICAL|       Europe|\n",
      "|2022-02-19 08:12:00| United States|           2.0|                         120.0|  Op_006|Channel_002|          PRO_1334|ORCHESTRA, CLASSICAL|North America|\n",
      "|2022-02-19 08:13:00|United Kingdom|           1.0|                          60.0|  Op_006|Channel_002|          PRO_1334|ORCHESTRA, CLASSICAL|       Europe|\n",
      "|2022-02-19 08:13:00| United States|           2.0|                         120.0|  Op_006|Channel_002|          PRO_1334|ORCHESTRA, CLASSICAL|North America|\n",
      "+-------------------+--------------+--------------+------------------------------+--------+-----------+------------------+--------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- time: timestamp (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- total_sessions: double (nullable = true)\n",
      " |-- total_session_duration_seconds: double (nullable = true)\n",
      " |-- operator: string (nullable = true)\n",
      " |-- channel: string (nullable = true)\n",
      " |-- cleaned_content_id: string (nullable = true)\n",
      " |-- genre: string (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the minute aggregation data\n",
    "\n",
    "minute_aggregation_data_df = spark.read.format(\"jdbc\").options(\n",
    "         url='jdbc:postgresql://qwest-final-project.ccngkdwtiuvz.us-east-2.rds.amazonaws.com:5432/Qwest-Database',\n",
    "         dbtable='cleaned_merged_minute_aggregation',\n",
    "         user='postgres',\n",
    "         password=password,\n",
    "         driver='org.postgresql.Driver').load()\n",
    "\n",
    "minute_aggregation_data_df.show()\n",
    "minute_aggregation_data_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put cleaned minute aggregation data in as PANDAS dataframe, as well as country to filter and start/end datetime in YYYY-MM-DD HH:mm:ss format\n",
    "def min_agg_trimmer(df, country, resample_by, start_datetime, end_datetime):\n",
    "    # Change string to datetime\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "    # drop extra index column\n",
    "    # df = df.drop(columns=df.columns[0], axis=1) (THIS WAS FOR LEGACY DATASET)\n",
    "    # loc to timeframe of advertising data\n",
    "    # Max supported for merging with advertising data is:\n",
    "    #df = df.loc[(df['time'] >= '2022-01-01 00:00:00') & (df['time'] <='2022-10-25 23:59:59')]\n",
    "    df = df.loc[(df['time'] >= start_datetime) & (df['time'] <= end_datetime)]\n",
    "    # drop columns\n",
    "    df = df.drop(['operator', 'channel', 'cleaned_content_id'], axis=1)\n",
    "    # filter by country\n",
    "    df = df.loc[(df['country'] == country)]\n",
    "    # remove space after comma for some genre entries for get dummies seperator\n",
    "    df['genre'] = df['genre'].str.replace(r'([ ]+,[ ]+)',\",\")\n",
    "    df['genre'] = df['genre'].str.strip()\n",
    "    # split genres with get dummies and drop genre column.  Clean data strip spaces from genre column names\n",
    "    df = pd.concat([df,df.genre.str.get_dummies(sep=',')],1)\n",
    "    df = df.drop('genre', axis=1)\n",
    "    df.columns = df.columns.str.strip()\n",
    "    # check column names\n",
    "    print(df.columns.values.tolist())\n",
    "    # filter by date and resample\n",
    "    df = df.loc[(df['time'] >= start_datetime) & (df['time'] <=end_datetime)]\n",
    "    df = df.resample(resample_by, on='time').sum().reset_index()\n",
    "    df = df.sort_values(['time'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'time'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\ML-GPU\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3629\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3628\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3629\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[0;32m   3630\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\ML-GPU\\lib\\site-packages\\pandas\\_libs\\index.pyx:136\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\ML-GPU\\lib\\site-packages\\pandas\\_libs\\index.pyx:163\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5198\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5206\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'time'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mf:\\Data Analytics\\Final Project\\prediction_ML_v3.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/Data%20Analytics/Final%20Project/prediction_ML_v3.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Can the ML model predict viewership based on genre and datetime?\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/Data%20Analytics/Final%20Project/prediction_ML_v3.ipynb#W3sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# lets try for UK\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/f%3A/Data%20Analytics/Final%20Project/prediction_ML_v3.ipynb#W3sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m uk_data \u001b[39m=\u001b[39m min_agg_trimmer(minute_aggregation_data_df\u001b[39m.\u001b[39;49mtoPandas(), \u001b[39m'\u001b[39;49m\u001b[39mUnited Kingdom\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mH\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39m2022-01-01 00:00:00\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39m2022-10-25 23:59:59\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/Data%20Analytics/Final%20Project/prediction_ML_v3.ipynb#W3sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m uk_data\n",
      "\u001b[1;32mf:\\Data Analytics\\Final Project\\prediction_ML_v3.ipynb Cell 4\u001b[0m in \u001b[0;36mmin_agg_trimmer\u001b[1;34m(df, country, resample_by, start_datetime, end_datetime)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/Data%20Analytics/Final%20Project/prediction_ML_v3.ipynb#W3sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m df \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mdrop(columns\u001b[39m=\u001b[39mdf\u001b[39m.\u001b[39mcolumns[\u001b[39m0\u001b[39m], axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/Data%20Analytics/Final%20Project/prediction_ML_v3.ipynb#W3sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# loc to timeframe of advertising data\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/Data%20Analytics/Final%20Project/prediction_ML_v3.ipynb#W3sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# Max supported for merging with advertising data is:\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/Data%20Analytics/Final%20Project/prediction_ML_v3.ipynb#W3sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m#df = df.loc[(df['time'] >= '2022-01-01 00:00:00') & (df['time'] <='2022-10-25 23:59:59')]\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/f%3A/Data%20Analytics/Final%20Project/prediction_ML_v3.ipynb#W3sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m df \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mloc[(df[\u001b[39m'\u001b[39;49m\u001b[39mtime\u001b[39;49m\u001b[39m'\u001b[39;49m] \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m start_datetime) \u001b[39m&\u001b[39m (df[\u001b[39m'\u001b[39m\u001b[39mtime\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m end_datetime)]\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Data%20Analytics/Final%20Project/prediction_ML_v3.ipynb#W3sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# drop columns\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Data%20Analytics/Final%20Project/prediction_ML_v3.ipynb#W3sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m df \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mdrop([\u001b[39m'\u001b[39m\u001b[39moperator\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mchannel\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mcleaned_content_id\u001b[39m\u001b[39m'\u001b[39m], axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\ML-GPU\\lib\\site-packages\\pandas\\core\\frame.py:3505\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3503\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   3504\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3505\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[0;32m   3506\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3507\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\ML-GPU\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3631\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3629\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3630\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m-> 3631\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m   3632\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m   3633\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3634\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3635\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3636\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'time'"
     ]
    }
   ],
   "source": [
    "# Can the ML model predict viewership based on genre? What happens if we add datetime?\n",
    "# lets try for UK\n",
    "uk_data = min_agg_trimmer(minute_aggregation_data_df.toPandas(), 'United Kingdom', 'H', '2022-01-01 00:00:00','2022-10-25 23:59:59')\n",
    "uk_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows where data is empty\n",
    "uk_data = uk_data[uk_data['total_sessions'] != 0]\n",
    "uk_data = uk_data.drop('total_sessions')\n",
    "#uk_data = uk_data.drop('time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST ROW\n",
    "feature_list = uk_data.columns.to_list()\n",
    "feature_list.remove('time', 'country', 'total_session_duration_seconds', 'region',)\n",
    "features = uk_data.drop([feature_list])\n",
    "features.index=uk_data['time']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the ML Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split our preprocessed data into our features and target arrays\n",
    "\n",
    "# ----------- FOR SEQUENTIAL MODEL ------------------------------------------\n",
    "# y = uk_data[\"total_session_duration_seconds\"].values\n",
    "# X = uk_data.drop([\"total_session_duration_seconds\"],1).values\n",
    "\n",
    "# # Split the preprocessed data into a training and testing dataset\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=69)\n",
    "#----------------------------------------------------------------------------\n",
    "\n",
    "#------------ FOR TIME SERIES MODEL -----------------------------------------\n",
    "split_fraction = 0.7\n",
    "train_split = int(split_fraction * int(uk_data.shape[0]))\n",
    "\n",
    "# can optimize these parameters later...\n",
    "learning_rate = 0.001\n",
    "batch_size = 256\n",
    "epochs = 50\n",
    "\n",
    "# configure test and training split\n",
    "feature_list = uk_data.columns.to_list()\n",
    "feature_list.remove('time', 'country', 'total_session_duration_seconds', 'region',)\n",
    "features = uk_data.drop([feature_list])\n",
    "features.index=uk_data['time']\n",
    "\n",
    "# def normalize(data, train_split):\n",
    "#     data_mean = data[:train_split].mean(axis=0)\n",
    "#     data_std = data[:train_split].std(axis=0)\n",
    "#     return (data - data_mean) / data_std\n",
    "\n",
    "# features = normalize(features.values, train_split)\n",
    "# features = pd.DataFrame(features)\n",
    "# features.head()\n",
    "\n",
    "train_data = features.loc[0 : train_split - 1]\n",
    "val_data = features.loc[train_split:]\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------ FOR TIME SERIES MODEL -----------------------------------------\n",
    "# prediction timeframe / observation timeframe ie: 1 hour / 10 minutes = 6\n",
    "step = 24\n",
    "# number of past timestamps used to train ie if 1 hour and 10 minute observation split, look back 120 hours is 120*6=720 or historic model timeframe * step\n",
    "past = 7*step\n",
    "# number of future timestamps to predict ie if we want to predict 12 hours into the future, 12*6=72 or future model timeframe * step\n",
    "future = 1*step\n",
    "\n",
    "start = past + future\n",
    "end = start + train_split\n",
    "\n",
    "X_train = train_data[[i for i in range(7)]].values\n",
    "y_train = features.iloc[start:end][[1]]\n",
    "\n",
    "sequence_length = int(past / step)\n",
    "#----------------------------------------------------------------------------\n",
    "\n",
    "# ----------- FOR SEQUENTIAL MODEL ------------------------------------------\n",
    "# # Define the model - deep neural net, i.e., the number of input features and hidden nodes for each layer.\n",
    "# input_features = len(X_train[0])\n",
    "# hidden_nodes_layer1 = 180\n",
    "# hidden_nodes_layer2 = 50\n",
    "# hidden_nodes_layer3 = 10\n",
    "\n",
    "# neural_network = tf.keras.models.Sequential()\n",
    "\n",
    "# # First hidden layer\n",
    "# neural_network.add(tf.keras.layers.Dense(units=hidden_nodes_layer1, input_dim=input_features, activation=\"relu\"))\n",
    "# # Second hidden layer\n",
    "# neural_network.add(tf.keras.layers.Dense(units=hidden_nodes_layer2, activation=\"relu\"))\n",
    "# # third hidden layer\n",
    "# neural_network.add(tf.keras.layers.Dense(units=hidden_nodes_layer3, activation=\"relu\"))\n",
    "\n",
    "# # Output layer\n",
    "# neural_network.add(tf.keras.layers.Dense(units=1))\n",
    "\n",
    "# # Check the structure of the model\n",
    "# neural_network.summary()\n",
    "# # Compile Model\n",
    "# neural_network.compile(loss= \"mean_squared_error\" , optimizer=\"adam\", metrics=[RSquare()])\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "\n",
    "#------------ FOR TIME SERIES MODEL -----------------------------------------\n",
    "# PreProcessing\n",
    "dataset_train = keras.preprocessing.timeseries_dataset_from_array(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    sequence_length=sequence_length,\n",
    "    sampling_rate=step,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "\n",
    "x_end = len(val_data) - past - future\n",
    "\n",
    "label_start = train_split + past + future\n",
    "\n",
    "x_val = val_data.iloc[:x_end][[i for i in range(7)]].values\n",
    "y_val = features.iloc[label_start:][[1]]\n",
    "\n",
    "dataset_val = keras.preprocessing.timeseries_dataset_from_array(\n",
    "    x_val,\n",
    "    y_val,\n",
    "    sequence_length=sequence_length,\n",
    "    sampling_rate=step,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "\n",
    "\n",
    "for batch in dataset_train.take(1):\n",
    "    inputs, targets = batch\n",
    "\n",
    "print(\"Input shape:\", inputs.numpy().shape)\n",
    "print(\"Target shape:\", targets.numpy().shape)\n",
    "\n",
    "# Training \n",
    "inputs = keras.layers.Input(shape=(inputs.shape[1], inputs.shape[2]))\n",
    "lstm_out = keras.layers.LSTM(32)(inputs)\n",
    "outputs = keras.layers.Dense(1)(lstm_out)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate), loss=\"mse\" , metric=['accuracy'])\n",
    "model.summary()\n",
    "#----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------- FOR SEQUENTIAL MODEL ------------------------------------------\n",
    "# fit model\n",
    "#neural_network.fit(X_train, y_train, epochs=500)\n",
    "#----------------------------------------------------------------------------\n",
    "\n",
    "# path_checkpoint = \"model_checkpoint.h5\"\n",
    "#es_callback = keras.callbacks.EarlyStopping(monitor=\"val_loss\", min_delta=0, patience=5)\n",
    "\n",
    "# modelckpt_callback = keras.callbacks.ModelCheckpoint(\n",
    "#     monitor=\"val_loss\",\n",
    "#     filepath=path_checkpoint,\n",
    "#     verbose=1,\n",
    "#     save_weights_only=True,\n",
    "#     save_best_only=True,\n",
    "# )\n",
    "\n",
    "#------------ FOR TIME SERIES MODEL -----------------------------------------\n",
    "history = model.fit(\n",
    "    dataset_train,\n",
    "    epochs=epochs,\n",
    "    validation_data=dataset_val,\n",
    "#    callbacks=[es_callback, modelckpt_callback],\n",
    ")\n",
    "#----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------- FOR SEQUENTIAL MODEL ------------------------------------------\n",
    "#check results\n",
    "# y_test_pred = neural_network.predict(X_test)\n",
    "# plt.plot(y_test)\n",
    "# plt.plot(y_test_pred)\n",
    "#----------------------------------------------------------------------------\n",
    "\n",
    "#------------ FOR TIME SERIES MODEL -----------------------------------------\n",
    "def visualize_loss(history, title):\n",
    "    loss = history.history[\"loss\"]\n",
    "    val_loss = history.history[\"val_loss\"]\n",
    "    epochs = range(len(loss))\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, loss, \"b\", label=\"Training loss\")\n",
    "    plt.plot(epochs, val_loss, \"r\", label=\"Validation loss\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "visualize_loss(history, \"Training and Validation Loss\")\n",
    "\n",
    "def show_plot(plot_data, delta, title):\n",
    "    labels = [\"History\", \"True Future\", \"Model Prediction\"]\n",
    "    marker = [\".-\", \"rx\", \"go\"]\n",
    "    time_steps = list(range(-(plot_data[0].shape[0]), 0))\n",
    "    if delta:\n",
    "        future = delta\n",
    "    else:\n",
    "        future = 0\n",
    "\n",
    "    plt.title(title)\n",
    "    for i, val in enumerate(plot_data):\n",
    "        if i:\n",
    "            plt.plot(future, plot_data[i], marker[i], markersize=10, label=labels[i])\n",
    "        else:\n",
    "            plt.plot(time_steps, plot_data[i].flatten(), marker[i], label=labels[i])\n",
    "    plt.legend()\n",
    "    plt.xlim([time_steps[0], (future + 5) * 2])\n",
    "    plt.xlabel(\"Time-Step\")\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "\n",
    "for x, y in dataset_val.take(5):\n",
    "    show_plot(\n",
    "        [x[0][:, 1].numpy(), y[0].numpy(), model.predict(x)[0]],\n",
    "        12,\n",
    "        \"Single Step Prediction\",\n",
    "    )\n",
    "#----------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('ML-GPU')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "448edaee4350f241a20dd5523d676348370318c49887029613f9e42feaa6bcd3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
